{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f45afd49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "import org.apache.spark.sql.functions._\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef495ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "address: Seq[(Int, String, String)] = List((1,14851 Washington RD,DE), (2,21821 Margarita ST,NY), (3,31311 Siemon Ave,CA))\r\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val address = Seq(\n",
    "    (1,\"14851 Washington RD\",\"DE\"),\n",
    "    (2,\"21821 Margarita ST\",\"NY\"),\n",
    "    (3,\"31311 Siemon Ave\",\"CA\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1de8686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----+\n",
      "| ID|            Address|State|\n",
      "+---+-------------------+-----+\n",
      "|  1|14851 Washington RD|   DE|\n",
      "|  2| 21821 Margarita ST|   NY|\n",
      "|  3|   31311 Siemon Ave|   CA|\n",
      "+---+-------------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_address: org.apache.spark.sql.DataFrame = [ID: int, Address: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_address = address.toDF(\"ID\",\"Address\",\"State\")\n",
    "df_address.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bd0bb8",
   "metadata": {},
   "source": [
    "# String Replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32f835e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+\n",
      "| ID|             Address|State|\n",
      "+---+--------------------+-----+\n",
      "|  1|14851 Washington ...|   DE|\n",
      "|  2|  21821 Margarita ST|   NY|\n",
      "|  3|    31311 Siemon Ave|   CA|\n",
      "+---+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_address.withColumn(\"Address\",regexp_replace($\"Address\",\"RD\",\"Road\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8e08ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+\n",
      "| ID|             Address|State|\n",
      "+---+--------------------+-----+\n",
      "|  1|14851 Washington ...|   DE|\n",
      "|  2|21821 Margarita Road|   NY|\n",
      "|  3|   31311 Siemon Road|   CA|\n",
      "+---+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_address.withColumn(\"Address\",regexp_replace($\"Address\",\"RD|ST|Ave\",\"Road\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abdd3060",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "2: error: invalid escape character\r",
     "output_type": "error",
     "traceback": [
      "<console>:2: error: invalid escape character\r",
      "       df_address.withColumn(\"Address\",regexp_replace($\"Address\",\"\\d+\",\"Road\")).show()\r",
      "                                                                   ^\r",
      ""
     ]
    }
   ],
   "source": [
    "df_address.withColumn(\"Address\",regexp_replace($\"Address\",\"\",\"Road\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d91c77",
   "metadata": {},
   "source": [
    "# When Otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "625de6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+\n",
      "| ID|             Address|State|\n",
      "+---+--------------------+-----+\n",
      "|  1|14851 Washington ...|   DE|\n",
      "|  2|21821 Margarita S...|   NY|\n",
      "|  3| 31311 Siemon Avenue|   CA|\n",
      "+---+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_address.withColumn(\"Address\", when($\"Address\".endsWith(\"RD\"),regexp_replace($\"Address\",\"RD\",\"Road\"))\n",
    "                     .when($\"Address\".endsWith(\"ST\"),regexp_replace($\"Address\",\"ST\",\"Street\"))\n",
    "                     .when($\"Address\".endsWith(\"Ave\"),regexp_replace($\"Address\",\"Ave\",\"Avenue\"))\n",
    "                     .otherwise(\"Unknown\")\n",
    "                     ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52fd6ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: List[(String, String, String, String, String, Int)] = List((Reyner,\"\",Wongso,0312,M,12000), (Robert,\"\",Williams,0412,\"\",13000), (Sekar,Alisha,Firdaus,0505,F,14000), (Richard,Mary,B,0404,X,10000))\r\n",
       "cols: Seq[String] = List(firstName, middleName, lastName, DOB, gender, salary)\r\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = List((\"Reyner\",\"\",\"Wongso\",\"0312\",\"M\",12000),\n",
    "               (\"Robert\",\"\",\"Williams\",\"0412\",\"\",13000),\n",
    "               (\"Sekar\",\"Alisha\",\"Firdaus\",\"0505\",\"F\",14000),\n",
    "               (\"Richard\",\"Mary\",\"B\",\"0404\",\"X\",10000))\n",
    "\n",
    "val cols = Seq(\"firstName\",\"middleName\",\"lastName\",\"DOB\",\"gender\",\"salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "268e1a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----+------+------+\n",
      "|firstName|middleName|lastName| DOB|gender|salary|\n",
      "+---------+----------+--------+----+------+------+\n",
      "|   Reyner|          |  Wongso|0312|     M| 12000|\n",
      "|   Robert|          |Williams|0412|      | 13000|\n",
      "|    Sekar|    Alisha| Firdaus|0505|     F| 14000|\n",
      "|  Richard|      Mary|       B|0404|     X| 10000|\n",
      "+---------+----------+--------+----+------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [firstName: string, middleName: string ... 4 more fields]\r\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.createDataFrame(data).toDF(cols:_*)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "acb1968b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----+------+------+---------+\n",
      "|firstName|middleName|lastName| DOB|gender|salary|newGender|\n",
      "+---------+----------+--------+----+------+------+---------+\n",
      "|   Reyner|          |  Wongso|0312|     M| 12000|     Male|\n",
      "|   Robert|          |Williams|0412|      | 13000|  Unknown|\n",
      "|    Sekar|    Alisha| Firdaus|0505|     F| 14000|  Unknown|\n",
      "|  Richard|      Mary|       B|0404|     X| 10000|  Unknown|\n",
      "+---------+----------+--------+----+------+------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df1: org.apache.spark.sql.DataFrame = [firstName: string, middleName: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df1 = df.withColumn(\"newGender\",when(col(\"gender\") === \"M\",\"Male\")\n",
    "                        .otherwise(\"Unknown\"))\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7663cc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----+------+------+---------+\n",
      "|firstName|middleName|lastName| DOB|gender|salary|newGender|\n",
      "+---------+----------+--------+----+------+------+---------+\n",
      "|   Reyner|          |  Wongso|0312|     M| 12000|     Male|\n",
      "|   Robert|          |Williams|0412|      | 13000|  Unknown|\n",
      "|    Sekar|    Alisha| Firdaus|0505|     F| 14000|   Female|\n",
      "|  Richard|      Mary|       B|0404|     X| 10000|  Unknown|\n",
      "+---------+----------+--------+----+------+------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [firstName: string, middleName: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df2 = df.withColumn(\"newGender\",when(col(\"gender\") === \"M\",\"Male\")\n",
    "                        .when(col(\"gender\") === \"F\",\"Female\")\n",
    "                        .otherwise(\"Unknown\"))\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395987b7",
   "metadata": {},
   "source": [
    "# Trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "edb51408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|col1|   col2|\n",
      "+----+-------+\n",
      "|   1|  ABC  |\n",
      "|   2|  ABC  |\n",
      "|   3|    ABC|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data: Seq[(Int, String)] = List((1,\"ABC  \"), (2,\"  ABC  \"), (3,\"   ABC\"))\r\n",
       "df: org.apache.spark.sql.DataFrame = [col1: int, col2: string]\r\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Seq((1,\"ABC  \"),\n",
    "               (2,\"  ABC  \"),\n",
    "              (3,\"   ABC\"))\n",
    "val df = data.toDF(\"col1\",\"col2\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9045ac70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   1| ABC|\n",
      "|   2| ABC|\n",
      "|   3| ABC|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"col2\",trim(col(\"col2\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f3f58d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|col1|  col2|\n",
      "+----+------+\n",
      "|   1|   ABC|\n",
      "|   2|   ABC|\n",
      "|   3|   ABC|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"col2\",rtrim(col(\"col2\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c66d5739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|col1| col2|\n",
      "+----+-----+\n",
      "|   1|ABC  |\n",
      "|   2|ABC  |\n",
      "|   3|  ABC|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"col2\",ltrim(col(\"col2\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3a634b",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b6fffc39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: Seq[(String, String, String, Int)] = List((Sharon, Zefanya, Setiawan,2003,F,10000), (Lintang, Diah, Ayuningtyas,2003,F,50000), (Matthew, Aaron, Sugiyarto,2003,M,20000))\r\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Seq((\"Sharon, Zefanya, Setiawan\",\"2003\",\"F\",10000),\n",
    "              (\"Lintang, Diah, Ayuningtyas\",\"2003\",\"F\",50000),\n",
    "              (\"Matthew, Aaron, Sugiyarto\",\"2003\",\"M\",20000)\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "adca3236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [Name: string, DOB Year: string ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = data.toDF(\"Name\",\"DOB Year\",\"Gender\",\"Salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c2f9d09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------+------+---------+----------+------------+\n",
      "|                Name|DOB Year|Gender|Salary|firstName|middleName|    lastName|\n",
      "+--------------------+--------+------+------+---------+----------+------------+\n",
      "|Sharon, Zefanya, ...|    2003|     F| 10000|   Sharon|   Zefanya|    Setiawan|\n",
      "|Lintang, Diah, Ay...|    2003|     F| 50000|  Lintang|      Diah| Ayuningtyas|\n",
      "|Matthew, Aaron, S...|    2003|     M| 20000|  Matthew|     Aaron|   Sugiyarto|\n",
      "+--------------------+--------+------+------+---------+----------+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "splitDf: org.apache.spark.sql.DataFrame = [Name: string, DOB Year: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val splitDf = (df.withColumn(\"firstName\",split(col(\"Name\"),\",\").getItem(0))\n",
    "              .withColumn(\"middleName\",split(col(\"Name\"),\",\").getItem(1))\n",
    "              .withColumn(\"lastName\",split(col(\"Name\"),\",\").getItem(2))\n",
    "              ) // get item means get index\n",
    "\n",
    "splitDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4eb8d7",
   "metadata": {},
   "source": [
    "# Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ff563e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "number: Seq[Double] = List(1.12, 13.0, 9.87, 2.34, 2.5, 3.5)\r\n",
       "numberDF: org.apache.spark.sql.DataFrame = [value: double]\r\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val number = Seq(1.12, 13, 9.87, 2.34, 2.5, 3.5)\n",
    "val numberDF = number.toDF(\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "25ce4e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|value|Round|\n",
      "+-----+-----+\n",
      "| 1.12|  1.0|\n",
      "| 13.0| 13.0|\n",
      "| 9.87| 10.0|\n",
      "| 2.34|  2.0|\n",
      "|  2.5|  3.0|\n",
      "|  3.5|  4.0|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numberDF.withColumn(\"Round\",round(col(\"value\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "94fc2dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res38: Seq[Long] = List(1, 13, 10, 2, 3, 4)\r\n"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number.map(x => x.round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3c0d4f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|value|Round|\n",
      "+-----+-----+\n",
      "| 1.12|  1.1|\n",
      "| 13.0| 13.0|\n",
      "| 9.87|  9.9|\n",
      "| 2.34|  2.3|\n",
      "|  2.5|  2.5|\n",
      "|  3.5|  3.5|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numberDF.withColumn(\"Round\",round(col(\"value\"),1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8ecc1d",
   "metadata": {},
   "source": [
    "# to_date Vs date_format\n",
    "- to_date = mengubah date yang belum standar untuk mengikuti standar (yyyy-MM-dd)\n",
    "- date_format = bisa mengubah format, jika formatnya sudah mengikuti standar (yyyy-MM-dd)\n",
    "\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b3a03cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+\n",
      "|Date    |Time                    |\n",
      "+--------+------------------------+\n",
      "|20201111|11-Nov-2020 11:53:01.123|\n",
      "|20211212|12-Dec-2021 10:52:05.131|\n",
      "|20220202|02-Feb-2022 00:00:00.000|\n",
      "+--------+------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetimes: Seq[(Int, String)] = List((20201111,11-Nov-2020 11:53:01.123), (20211212,12-Dec-2021 10:52:05.131), (20220202,02-Feb-2022 00:00:00.000))\r\n",
       "df: org.apache.spark.sql.DataFrame = [Date: int, Time: string]\r\n"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datetimes = Seq((20201111,\"11-Nov-2020 11:53:01.123\"),\n",
    "                   (20211212,\"12-Dec-2021 10:52:05.131\"),\n",
    "                   (20220202,\"02-Feb-2022 00:00:00.000\"))\n",
    "\n",
    "val df = datetimes.toDF(\"Date\",\"Time\")\n",
    "df.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800dd2f7",
   "metadata": {},
   "source": [
    "# to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c1e30285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+----------+-----------------------+\n",
      "|Date    |Time                    |toDate    |toTime                 |\n",
      "+--------+------------------------+----------+-----------------------+\n",
      "|20201111|11-Nov-2020 11:53:01.123|2020-11-11|2020-11-11 11:53:01.123|\n",
      "|20211212|12-Dec-2021 10:52:05.131|2021-12-12|2021-12-12 10:52:05.131|\n",
      "|20220202|02-Feb-2022 00:00:00.000|2022-02-02|2022-02-02 00:00:00    |\n",
      "+--------+------------------------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfNew: org.apache.spark.sql.DataFrame = [Date: int, Time: string ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\"toDate\", to_date(col(\"Date\").cast(\"String\"),\"yyyyMMdd\"))\n",
    ".withColumn(\"toTime\",to_timestamp(col(\"Time\"),\"dd-MMM-yyyy HH:mm:ss.SSS\")\n",
    ".show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1cbf7b",
   "metadata": {},
   "source": [
    "# date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ea60014b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------------+\n",
      "|Date      |Time                    |\n",
      "+----------+------------------------+\n",
      "|2020-11-11|11-Nov-2020 11:53:01.123|\n",
      "|2021-12-12|12-Dec-2021 10:52:05.131|\n",
      "|2022-2-2  |2-Feb-2022 00:00:00.000 |\n",
      "+----------+------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetimes: Seq[(String, String)] = List((2020-11-11,11-Nov-2020 11:53:01.123), (2021-12-12,12-Dec-2021 10:52:05.131), (2022-2-2,2-Feb-2022 00:00:00.000))\r\n",
       "df: org.apache.spark.sql.DataFrame = [Date: string, Time: string]\r\n"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datetimes = Seq((\"2020-11-11\",\"11-Nov-2020 11:53:01.123\"),\n",
    "                   (\"2021-12-12\",\"12-Dec-2021 10:52:05.131\"),\n",
    "                   (\"2022-2-2\",\"2-Feb-2022 00:00:00.000\"))\n",
    "\n",
    "val df = datetimes.toDF(\"Date\",\"Time\")\n",
    "df.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b56852ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------------+------+\n",
      "|Date      |Time                    |toDate|\n",
      "+----------+------------------------+------+\n",
      "|2020-11-11|11-Nov-2020 11:53:01.123|2020  |\n",
      "|2021-12-12|12-Dec-2021 10:52:05.131|2021  |\n",
      "|2022-2-2  |2-Feb-2022 00:00:00.000 |2022  |\n",
      "+----------+------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"toDate\",date_format(col(\"Date\"),\"yyyy\"))\n",
    "// .withColumn(\"toTime\",date_format(col(\"Time\"),\"yyyy\"))\n",
    ".show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "78479e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|Date      |Time                   |\n",
      "+----------+-----------------------+\n",
      "|2020-11-11|2020-11-11 11:53:01.123|\n",
      "|2021-12-12|2021-12-12 10:52:05.131|\n",
      "|2022-2-2  |2022-2-2 00:00:00.000  |\n",
      "+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetimes: Seq[(String, String)] = List((2020-11-11,2020-11-11 11:53:01.123), (2021-12-12,2021-12-12 10:52:05.131), (2022-2-2,2022-2-2 00:00:00.000))\r\n",
       "df: org.apache.spark.sql.DataFrame = [Date: string, Time: string]\r\n"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datetimes = Seq((\"2020-11-11\",\"2020-11-11 11:53:01.123\"),\n",
    "                   (\"2021-12-12\",\"2021-12-12 10:52:05.131\"),\n",
    "                   (\"2022-2-2\",\"2022-2-2 00:00:00.000\"))\n",
    "\n",
    "val df = datetimes.toDF(\"Date\",\"Time\")\n",
    "df.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "60621538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+--------------+--------------+\n",
      "|Date      |Time                   |toDate        |toTime        |\n",
      "+----------+-----------------------+--------------+--------------+\n",
      "|2020-11-11|2020-11-11 11:53:01.123|20201111000000|20201111115301|\n",
      "|2021-12-12|2021-12-12 10:52:05.131|20211212000000|20211212105205|\n",
      "|2022-2-2  |2022-2-2 00:00:00.000  |20220202000000|20220202000000|\n",
      "+----------+-----------------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"toDate\",date_format(col(\"Date\"),\"yyyyMMddHHmmss\"))\n",
    ".withColumn(\"toTime\",date_format(col(\"Time\"),\"yyyyMMddHHmmss\"))\n",
    ".show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9fec05e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-----------------+\n",
      "|Date      |Time                   |toDate           |\n",
      "+----------+-----------------------+-----------------+\n",
      "|2020-11-11|2020-11-11 11:53:01.123|November 11, 2020|\n",
      "|2021-12-12|2021-12-12 10:52:05.131|December 12, 2021|\n",
      "|2022-2-2  |2022-2-2 00:00:00.000  |February 2, 2022 |\n",
      "+----------+-----------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"toDate\",date_format(col(\"Date\"),\"MMMM d, yyyy\"))\n",
    ".show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "456273dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------------+------+\n",
      "|Date      |Time                    |toDate|\n",
      "+----------+------------------------+------+\n",
      "|2020-11-11|11-Nov-2020 11:53:01.123|Wed   |\n",
      "|2021-12-12|12-Dec-2021 10:52:05.131|Sun   |\n",
      "|2022-2-2  |2-Feb-2022 00:00:00.000 |Wed   |\n",
      "+----------+------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"toDate\",date_format(col(\"Date\"),\"E\"))\n",
    ".show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1d0c4c",
   "metadata": {},
   "source": [
    "# date_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3fa3f2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+----------+----------+----------+----------+\n",
      "|Date      |Time                   |addDate   |addTime   |subDate   |subTime   |\n",
      "+----------+-----------------------+----------+----------+----------+----------+\n",
      "|2020-11-11|2020-11-11 11:53:01.123|2020-11-16|2020-11-16|2020-11-06|2020-11-06|\n",
      "|2021-12-12|2021-12-12 10:52:05.131|2021-12-17|2021-12-17|2021-12-07|2021-12-07|\n",
      "|2022-2-2  |2022-2-2 00:00:00.000  |2022-02-07|2022-02-07|2022-01-28|2022-01-28|\n",
      "+----------+-----------------------+----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"addDate\",date_add(col(\"Date\"),5))\n",
    ".withColumn(\"addTime\", date_add(col(\"Time\"),5))\n",
    ".withColumn(\"subDate\",date_sub(col(\"Date\"),5))\n",
    ".withColumn(\"subTime\",date_sub(col(\"Time\"),5))\n",
    ".show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80ba636",
   "metadata": {},
   "source": [
    "# date_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ac723951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+--------------+--------------+\n",
      "|Date      |Time                   |dateDifference|timeDifference|\n",
      "+----------+-----------------------+--------------+--------------+\n",
      "|2020-11-11|2020-11-11 11:53:01.123|729           |729           |\n",
      "|2021-12-12|2021-12-12 10:52:05.131|333           |333           |\n",
      "|2022-2-2  |2022-2-2 00:00:00.000  |281           |281           |\n",
      "+----------+-----------------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"dateDifference\",datediff(current_date(),col(\"Date\")))\n",
    ".withColumn(\"timeDifference\",datediff(current_timestamp(),col(\"Time\")))\n",
    ".show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b82654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
